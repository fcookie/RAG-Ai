{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# RAG Document Assistant - Example Usage\n",
    "\n",
    "This notebook demonstrates how to use the RAG Document Assistant programmatically."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Setup and Configuration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "from pathlib import Path\n",
    "import time\n",
    "\n",
    "# Add project to path\n",
    "sys.path.append(str(Path.cwd().parent))\n",
    "\n",
    "from src.config import Config\n",
    "from src.document_processor import DocumentProcessor\n",
    "from src.vector_store import VectorStore, HybridRetriever\n",
    "from src.rag_chain import RAGChain\n",
    "from src.evaluator import RAGEvaluator, TestQuestionGenerator"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Initialize System"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize components\n",
    "vector_store = VectorStore(collection_name=\"demo_collection\")\n",
    "rag_chain = RAGChain(vector_store)\n",
    "evaluator = RAGEvaluator()\n",
    "\n",
    "print(\"✅ System initialized!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Process and Index Documents"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Process a document\n",
    "processor = DocumentProcessor()\n",
    "\n",
    "# Example: Process a PDF\n",
    "pdf_path = Path(\"../data/uploads/sample_paper.pdf\")\n",
    "\n",
    "if pdf_path.exists():\n",
    "    documents = processor.process_pdf(pdf_path)\n",
    "    print(f\"Processed {len(documents)} chunks from {pdf_path.name}\")\n",
    "    \n",
    "    # Add to vector store\n",
    "    vector_store.add_documents(documents)\n",
    "    print(\"✅ Documents indexed!\")\n",
    "else:\n",
    "    print(\"⚠️ Sample PDF not found. Please add a PDF to data/uploads/\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Query the System"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Ask a question\n",
    "question = \"What are the main findings of this research?\"\n",
    "\n",
    "print(f\"Question: {question}\\n\")\n",
    "\n",
    "start_time = time.time()\n",
    "response = rag_chain.query(question, search_mode=\"hybrid\")\n",
    "response_time = time.time() - start_time\n",
    "\n",
    "print(f\"Answer: {response.answer}\\n\")\n",
    "print(f\"Confidence: {response.confidence:.2%}\")\n",
    "print(f\"Response Time: {response_time:.2f}s\\n\")\n",
    "\n",
    "print(\"Citations:\")\n",
    "for i, citation in enumerate(response.citations, 1):\n",
    "    print(f\"{i}. Source: {citation.source}, Page: {citation.page}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Test Different Search Modes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "search_modes = [\"vector\", \"keyword\", \"hybrid\"]\n",
    "question = \"What methodology was used in the study?\"\n",
    "\n",
    "results = {}\n",
    "\n",
    "for mode in search_modes:\n",
    "    start = time.time()\n",
    "    response = rag_chain.query(question, search_mode=mode)\n",
    "    elapsed = time.time() - start\n",
    "    \n",
    "    results[mode] = {\n",
    "        'answer_length': len(response.answer),\n",
    "        'num_citations': len(response.citations),\n",
    "        'confidence': response.confidence,\n",
    "        'time': elapsed\n",
    "    }\n",
    "    \n",
    "    print(f\"\\n{mode.upper()} Search:\")\n",
    "    print(f\"  Answer Length: {results[mode]['answer_length']} chars\")\n",
    "    print(f\"  Citations: {results[mode]['num_citations']}\")\n",
    "    print(f\"  Confidence: {results[mode]['confidence']:.2%}\")\n",
    "    print(f\"  Time: {results[mode]['time']:.2f}s\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Evaluate Response Quality"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Evaluate a response\n",
    "question = \"What are the limitations of the study?\"\n",
    "\n",
    "start_time = time.time()\n",
    "response = rag_chain.query(question)\n",
    "response_time = time.time() - start_time\n",
    "\n",
    "print(f\"Answer: {response.answer[:200]}...\\n\")\n",
    "\n",
    "# Run evaluation\n",
    "metrics = evaluator.evaluate_response(question, response, response_time)\n",
    "\n",
    "print(\"\\nEvaluation Metrics:\")\n",
    "print(f\"  Answer Relevance: {metrics.answer_relevance:.2%}\")\n",
    "print(f\"  Citation Accuracy: {metrics.citation_accuracy:.2%}\")\n",
    "print(f\"  Faithfulness: {metrics.faithfulness:.2%}\")\n",
    "print(f\"  Retrieval Precision: {metrics.retrieval_precision:.2%}\")\n",
    "print(f\"  Response Time: {metrics.response_time:.2f}s\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7. Compare Multiple Documents"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Assuming you have multiple documents indexed\n",
    "stats = vector_store.get_collection_stats()\n",
    "print(f\"Total documents: {stats['unique_documents']}\")\n",
    "\n",
    "if stats['unique_documents'] >= 2:\n",
    "    # Get document IDs\n",
    "    all_docs = vector_store.get_all_documents()\n",
    "    doc_ids = list(set([doc.metadata.get('doc_id') for doc in all_docs if doc.metadata.get('doc_id')]))\n",
    "    \n",
    "    if len(doc_ids) >= 2:\n",
    "        question = \"Compare the methodologies used in these studies\"\n",
    "        response = rag_chain.compare_documents(question, doc_ids[:2])\n",
    "        \n",
    "        print(f\"Comparison Answer:\\n{response.answer}\")\n",
    "else:\n",
    "    print(\"Need at least 2 documents for comparison. Upload more documents first.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 8. Generate Test Questions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generate test questions from document\n",
    "generator = TestQuestionGenerator()\n",
    "\n",
    "# Get a sample document\n",
    "sample_docs = vector_store.get_all_documents()\n",
    "if sample_docs:\n",
    "    sample_text = sample_docs[0].page_content\n",
    "    \n",
    "    questions = generator.generate_questions_from_document(sample_text, num_questions=5)\n",
    "    \n",
    "    print(\"Generated Test Questions:\")\n",
    "    for i, q in enumerate(questions, 1):\n",
    "        print(f\"{i}. {q}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 9. Batch Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Evaluate multiple questions\n",
    "test_questions = [\n",
    "    \"What is the main hypothesis?\",\n",
    "    \"What data was collected?\",\n",
    "    \"What are the key conclusions?\"\n",
    "]\n",
    "\n",
    "all_metrics = []\n",
    "\n",
    "for question in test_questions:\n",
    "    start_time = time.time()\n",
    "    response = rag_chain.query(question)\n",
    "    response_time = time.time() - start_time\n",
    "    \n",
    "    metrics = evaluator.evaluate_response(question, response, response_time)\n",
    "    all_metrics.append(metrics)\n",
    "    \n",
    "    print(f\"\\nQ: {question}\")\n",
    "    print(f\"Relevance: {metrics.answer_relevance:.2%} | \"\n",
    "          f\"Citations: {metrics.citation_accuracy:.2%} | \"\n",
    "          f\"Time: {metrics.response_time:.2f}s\")\n",
    "\n",
    "# Get average metrics\n",
    "avg_metrics = evaluator.get_average_metrics()\n",
    "print(\"\\n=== Average Metrics ===\")\n",
    "for key, value in avg_metrics.items():\n",
    "    print(f\"{key}: {value:.2%}\" if 'time' not in key else f\"{key}: {value:.2f}s\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 10. Export Evaluation Report"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save evaluation report\n",
    "report_path = \"../data/evaluation_report.json\"\n",
    "evaluator.save_evaluation_report(report_path)\n",
    "\n",
    "print(f\"✅ Evaluation report saved to {report_path}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 11. View Collection Statistics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "stats = vector_store.get_collection_stats()\n",
    "\n",
    "print(\"=== Collection Statistics ===\")\n",
    "print(f\"Collection Name: {stats['collection_name']}\")\n",
    "print(f\"Total Chunks: {stats['total_chunks']}\")\n",
    "print(f\"Unique Documents: {stats['unique_documents']}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 12. Clean Up (Optional)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Uncomment to clear the collection\n",
    "# vector_store.clear_collection()\n",
    "# print(\"✅ Collection cleared!\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
